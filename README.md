可爱的小抓抓
====

Python爬虫框架Scrapy
---

###Scrapy框架
>最近在做一个叫做TianTian的Django项目，数据是通过爬虫获取的，爬虫用的框架就是Scrapy</br>
一开始我是想，用到了再查，毕竟主要是想做东西，爬虫框架当作工具用到的时候再翻一翻就行。然而，在操作的过程中发现不经过系统的学习，效率实在低了太多。</br>
所谓工欲善其事必先利其器，在浪费了大量的时间在研究Scrapy零碎模块的用法上之后，决定还是从文档看起，从示例看起，然后再应用到自己的项目上.</br>
>爬虫的办法有很多，除了框架以外，我们还可以利用一些优秀的工具，比如BeautifulSoup和requests结合，但是成熟的框架必定有值得我们学习的地方，熟悉一下是很有必要的。^_^</br>
####资源传送门：[Scrapy1.0文档](https://scrapy-chs.readthedocs.org/zh_CN/1.0/)

###03.28更新
>今天效率比较高，代码部分基本已经更完了.
>资源传送门：[Xpath教程](http://www.w3school.com.cn/xpath/)

###03.29更新
>今天更新了CrawlSpider这个类的示例
>这个类主要有以下特点:
- 有一个rule元组 可以满足我们在一个页面内抓取我们想要的链接
- rule元组的执行顺序是从上倒下依次执行
- Rule对象可以为指定格式的url定义回调函数和一些其它的规则
- Rule参数里面如果设置了follow
  意味着在捕获到的页面里面也要执行一遍rule元组里面定义的所有抓取规则
- 像这个例子里面 我们设置page的follow为True 也就是持续跟进 
- 实践证明跟进的时候捕获的url不会和前面已经捕获过的重复
